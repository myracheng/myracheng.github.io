## Publications
1. Myra Cheng, Alicia DeVrio, Lisa Egede, Su Lin Blodgett, Alexandra Olteanu. [Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI.](https://arxiv.org/pdf/2410.08526) ICLR 2025, Blogposts Track (forthcoming)

2. Alicia DeVrio, Myra Cheng, Lisa Egede, Alexandra Olteanu, Su Lin Blodgett. A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies. CHI 2025 (forthcoming)
   
1. Kristina Gligorić, Myra Cheng, Lucia Zheng, Esin Durmus, Dan Jurafsky. [NLP Systems That Can’t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps.](https://arxiv.org/pdf/2404.01651.pdf) NAACL 2024.

2. Myra Cheng, Kristina Gligorić, Tiziano Piccardi, Dan Jurafsky. [AnthroScore: A Computational Linguistic Measure of Anthropomorphism.](https://arxiv.org/pdf/2402.02056.pdf) EACL 2024. <span class="subline">[[website]](http://anthroscore.stanford.edu/)  [[slides]](anthroslides.pdf) [[code]](https://github.com/myracheng/AnthroScore)</span>  
<span class="subline">
Press coverage by [Scientific American](https://www.scientificamerican.com/article/can-ai-replace-human-research-participants-these-scientists-see-risks/) and [New Scientist](https://www.newscientist.com/article/2417992-researchers-increasingly-view-tech-as-having-human-like-qualities/).</span>  

3. Myra Cheng, Tiziano Piccardi, Diyi Yang. [CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations.](https://arxiv.org/pdf/2310.11501.pdf) EMNLP 2023. <span class="subline">[[poster]](compost_poster.pdf) [[code]](https://github.com/myracheng/lm_caricature)</span>

4. Myra Cheng, Esin Durmus, Dan Jurafsky. [Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models.](https://arxiv.org/pdf/2305.18189.pdf) ACL 2023. <span class="subline">[[slides]](marked_slides.pdf)  [[code]](https://github.com/myracheng/markedpersonas)</span>  
<span class="subline">Social Impact Award, Nominated for Best Paper</span> 

5. Federico Bianchi\*, Pratyusha Kalluri\*, Esin Durmus\*, Faisal Ladhak\*, Myra Cheng\*, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, Aylin Caliskan. [Easily accessible text-to-image generation amplifies demographic stereotypes at large scale.](https://arxiv.org/pdf/2211.03759.pdf) FAccT 2023.  
<span  class="subline">Press coverage by [Nature](https://www.nature.com/articles/d41586-024-00674-9), [CBS News Prime Time](https://www.youtube.com/watch?v=0KFJf9QqfCw), the [Washington Post](https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/) and [MIT Technology Review](https://www.technologyreview.com/2023/03/22/1070167/these-news-tool-let-you-see-for-yourself-how-biased-ai-image-models-are/).</span>

6. Myra Cheng, Maria De-Arteaga, Lester Mackey, Adam Tauman Kalai. [Social Norm Bias: Residual Harms of Fairness-Aware Algorithms.](https://arxiv.org/pdf/2108.11056.pdf) Data Mining and Knowledge Discovery 2023. <span class="subline">[[code]](https://github.com/pinkvelvet9/snobpaper/)</span>

7. Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng et al. [Ethical and Social Risks of Harm from Language Models.](https://arxiv.org/pdf/2112.04359.pdf) FAccT 2022.

### Preprints

1. Myra Cheng\*, Angela Y. Lee\*, Kristina Rapuano, Kate Niederhoffer, Alex Liebscher, Jeffrey Hancock. [From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors.](https://www.arxiv.org/pdf/2501.18045)

2. Pratyusha Ria Kalluri\*, William Agnew\*, Myra Cheng\*, Kentrell Owens\*, Luca Soldaini\*, Abeba Birhane\*. [The Surveillance AI Pipeline.](https://arxiv.org/pdf/2309.15084.pdf)   
<span  class="subline">Press coverage by [404 Media](https://www.404media.co/how-the-surveillance-ai-pipeline-literally-objectifies-human-beings/).</span>

### Also see [Semantic Scholar](https://www.semanticscholar.org/author/M.-Cheng/2149615775) & [Google Scholar](https://scholar.google.com/citations?user=gaslQl8AAAAJ&hl=en) :) 


<br><br>
